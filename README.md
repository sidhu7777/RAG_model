# RAG-Based Financial Query System

This repository contains a complete implementation of a **RAG (Retrieval-Augmented Generation)** pipeline designed to extract and analyze financial data from PDFs and provide natural language answers to user queries. The system combines state-of-the-art NLP techniques with a structured modular architecture for data extraction, preprocessing, embedding generation, vector database creation, and response generation.

## **Table of Contents**

1. [Overview](#overview)
2. [Key Features](#key-features)
3. [Project Structure](#project-structure)
4. [Dependencies](#dependencies)
5. [Setup Instructions](#setup-instructions)
6. [Pipeline Stages](#pipeline-stages)
7. [Error Handling and Debugging](#error-handling-and-debugging)
8. [Deployment](#deployment)
9. [Module Details](#module-details)
10. [Contributing](#contributing)
11. [License](#license)

---

## **Overview**

This project aims to build a robust system for querying financial documents using modern natural language processing (NLP) techniques. By leveraging a RAG approach, the system efficiently retrieves relevant content from structured and unstructured data stored in financial PDFs and provides answers generated by a pre-trained language model.

### **Use Cases**

- Analyze Profit & Loss statements, balance sheets, or other financial documents.
- Query revenue, expenses, or other specific metrics.
- Extract insights from tabular and textual data.

---

## **Key Features**

- **Modular Codebase**: Components for each pipeline stage, designed for reusability and scalability.
- **Preprocessing**: Handles raw text and table data, including normalization, tokenization, and chunking.
- **Embeddings and Vector Database**: Uses Sentence-BERT embeddings stored in a FAISS index for fast similarity search.
- **Retrieval-Augmented Generation**: Combines retrieval from FAISS with Hugging Face models for precise and contextual responses.
- **Customizable Parameters**: Allows easy tuning of thresholds, model types, and data paths.
- **Robust Logging and Error Handling**: Tracks every stage of the pipeline for debugging.

---

## **Project Structure**

The repository is organized as follows:

```
project_root/
├── src/
│   ├── components/
│   │   ├── data_extraction.py       # Extracts text and tables from PDFs
│   │   ├── data_preprocessing.py    # Cleans and normalizes text and table data
│   │   ├── new_vector.py            # Handles embeddings and FAISS index creation
│   │   ├── retriever.py             # Retrieves relevant content from FAISS
│   │   └── response_generator.py    # Generates responses using Hugging Face models
│   ├── logger.py                    # Logging utility
│   ├── exception.py                 # Custom exception handling
├── data/
│   ├── raw/                         # Raw data files (text and tables)
│   ├── processed/                   # Preprocessed data
│   │   ├── text/                    # Cleaned text chunks
│   │   └── flattened_table/         # Flattened tables
├── dynamic_pipeline/
│   └── main.py                      # Dynamic version of the pipeline for customization
├── notebook/
│   └── Sample Financial Statement.pdf  # Example financial document
├── README.md                        # Project documentation
├── requirements.txt                 # Dependencies
└── Dockerfile                       # Containerization setup
```

---

## **Dependencies**

The project relies on the following key libraries and tools:

- **Python 3.9+**
- **PDFPlumber**: Extracts text and tables from PDFs.
- **Camelot**: Parses tabular data from PDFs with precision.
- **Pandas**: Processes tabular data.
- **NLTK**: Text cleaning and tokenization.
- **WordNinja**: Splits concatenated words.
- **Sentence-BERT**: Generates embeddings for textual and tabular data.
- **FAISS**: Efficient similarity search and indexing.
- **Transformers (Hugging Face)**: For response generation using pre-trained models.
- **Streamlit**: To build the interactive interface.

---

## **Setup Instructions**

1. **Clone the Repository**:

   ```bash
   git clone <repository_url>
   cd project_root
   ```

2. **Install Dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

3. **Download NLTK Data**:

   ```python
   import nltk
   nltk.download('stopwords')
   ```

4. **Prepare Data**:

   - Place raw PDFs in the `data/raw/` directory.
   - Ensure the structure for raw text (`data/raw/text/`) and tables (`data/raw/table/`).

5. **Run the Pipeline**:

   - For local development:
     ```bash
     python src/main.py
     ```
   - For dynamic pipeline testing:
     ```bash
     python dynamic_pipeline/main.py
     ```

---

## **Pipeline Stages**

### **1. Data Extraction**

- Extracts text and table data from PDFs using `pdfplumber` and `camelot`.
- Outputs text files and CSVs into the `data/raw/` directory.

### **2. Data Preprocessing**

- Text:
  - Cleans and tokenizes raw text.
  - Splits concatenated words using `wordninja`.
  - Creates bi-grams and tri-grams.
  - Saves cleaned chunks in JSON format.
- Tables:
  - Handles missing values.
  - Flattens nested structures.

### **3. Embedding Generation and FAISS Indexing**

- Generates embeddings for cleaned text and table rows using Sentence-BERT.
- Combines embeddings into a FAISS index for fast similarity search.
- Saves unified mappings and embeddings for validation.

### **4. Retrieval**

- Retrieves the top-k relevant results based on the query's similarity score.
- Filters out low-similarity results to improve relevance.

### **5. Response Generation**

- Generates natural language answers using Hugging Face's GPT-2.
- Includes fallback logic for empty or irrelevant contexts.

---

## **Error Handling and Debugging**

- **Logging**: Comprehensive logs track every stage of the pipeline for debugging.
- **Custom Exceptions**: Each module has tailored exception handling to manage errors gracefully.
- **Common Issues Addressed**:
  - **WordNinja Compatibility**: The library was not directly compatible with the project environment. This was resolved by cloning the library from an external GitHub repository, placing it in the project directory, and adding it to the Python path using `sys.path.insert`.
  - **Mismatched Embeddings and Mappings**: Resolved with validation checks to ensure the counts were consistent before creating the FAISS index.
  - **Low Similarity Scores**: Tuned the `similarity_threshold` parameter to improve retrieval accuracy.
  - **Empty Contexts**: Added fallback responses for cases where no relevant data was retrieved.
  - **Library Import Errors**: Fixed compatibility issues by specifying precise library versions in `requirements.txt` and testing the environment.
  - **Path Errors**: Ensured absolute or relative paths were correctly resolved, especially for dynamic and static pipelines.
  - **Streamlit App Development**:
    - Addressed performance issues by implementing caching for frequently used data.
    - Debugged layout rendering to handle large responses and ensure compatibility with Streamlit Cloud.
  - **Python Version Mismatches**: Managed compatibility with different Python versions and deprecated methods by introducing fallback logic and updating dependencies.
  - **Debugging Data Processing**: Added detailed logs and exceptions to troubleshoot errors during text and table data cleaning and chunking.

---

## **Deployment**

### **Streamlit Cloud Deployment**

1. **Push the Repository to GitHub**:
   Ensure all changes are committed and pushed to your GitHub repository.

2. **Deploy the Streamlit App**:

   - Go to [Streamlit Cloud](https://streamlit.io/cloud).
   - Connect your GitHub repository.
   - Follow the on-screen instructions to deploy the app directly.

---

## **Module Details**

#### **1. `data_extraction.py`**
- **Purpose**: Extracts text and table data from financial PDFs using a combination of text parsing and tabular data extraction.
- **Key Features**:
  - Splits text and table content for separate processing.
  - Saves extracted data into raw folders for later preprocessing.
- **Status**: Appears complete and well-structured.

---

#### **2. `data_preprocessing.py`**
- **Purpose**: Preprocesses extracted text and tables for embedding generation.
- **Key Features**:
  - Cleans and tokenizes text using `WordNinja` and NLTK.
  - Handles missing values in tabular data.
  - Generates cleaned text chunks and flattened table files for embedding.
- **Status**: Fully implemented. Ready for use in the pipeline.

---

#### **3. `new_vector.py`**
- **Purpose**: Handles the creation and management of the vector database (FAISS).
- **Key Features**:
  - Generates embeddings for both text and table data.
  - Combines embeddings into a unified mapping.
  - Creates and stores the FAISS index.
- **Status**: Complete with detailed error handling and logging.

---

#### **4. `retriever.py`**
- **Purpose**: Retrieves relevant content from the FAISS index based on query embeddings.
- **Key Features**:
  - Uses Sentence Transformers for embedding queries.
  - Retrieves top-k relevant chunks while applying similarity thresholds.
  - Handles both text and table content gracefully.
- **Status**: Fully functional. Logs and exception handling are robust.

---

#### **5. `response_generator.py`**
- **Purpose**: Generates natural language answers to queries using Hugging Face models (e.g., GPT2).
- **Key Features**:
  - Builds a response based on retrieved contexts.
  - Allows fine-tuning of generation parameters (e.g., temperature, max length).
  - Fallback logic for empty or irrelevant contexts.
- **Status**: Complete and integrates well with the retriever.

---

#### **6. `main.py`**
- **Purpose**: Integrates all components into a complete pipeline.
- **Key Features**:
  - Executes all steps sequentially: extraction → preprocessing → embedding → retrieval → response generation.
  - Includes debugging logs to track progress and issues.
  - Demonstrates query-response functionality with a sample query.
- **Status**: Fully functional for backend integration and testing.

---

#### **7. `exception.py`**
- **Purpose**: Defines a custom exception class for consistent error handling across all modules.
- **Status**: Complete and effectively used throughout the project.

---

#### **8. `logger.py`**
- **Purpose**: Configures logging for the entire project.
- **Key Features**:
  - Provides logging levels for debug, info, warning, and errors.
  - Centralized logging improves traceability.
- **Status**: Well-implemented.

---

#### **9. `Sample Financial Statement.pdf`**
- **Purpose**: A sample input file for testing the pipeline.
- **Status**: Useful for validating functionality. Should be retained for demonstrations.

---





